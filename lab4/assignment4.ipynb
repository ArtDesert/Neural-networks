{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 4\n",
    "# Tensorflow 2.x\n",
    "# 6408 Дробин Роман\n",
    "# Вариант 2 (mnist dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Подготовка данных\n",
    "\n",
    "2) Использование Keras Model API\n",
    "\n",
    "3) Использование Keras Sequential + Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения лабораторной работы необходимо установить tensorflow версии 2.0 или выше .\n",
    "\n",
    "Рекомендуется использовать возможности Colab'а по обучению моделей на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных\n",
    "Загрузите набор данных из предыдущей лабораторной работы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 28, 28, 1)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 28, 28, 1)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 28, 28, 1)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_mnist(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the MNIST dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw Mnist dataset and use appropriate data types and shapes\n",
    "    mnist = tf.keras.datasets.mnist.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = mnist\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist()\n",
    "X_train = X_train.reshape(49000, 28, 28, 1)\n",
    "X_val = X_val.reshape(1000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 28, 28, 1) (64,)\n",
      "1 (64, 28, 28, 1) (64,)\n",
      "2 (64, 28, 28, 1) (64,)\n",
      "3 (64, 28, 28, 1) (64,)\n",
      "4 (64, 28, 28, 1) (64,)\n",
      "5 (64, 28, 28, 1) (64,)\n",
      "6 (64, 28, 28, 1) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keras Model Subclassing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Для реализации собственной модели с помощью Keras Model Subclassing API необходимо выполнить следующие шаги:\n",
    "\n",
    "1) Определить новый класс, который является наследником tf.keras.Model.\n",
    "\n",
    "2) В методе \\_\\_init\\_\\_() определить все необходимые слои из модуля tf.keras.layer\n",
    "\n",
    "3) Реализовать прямой проход в методе call() на основе слоев, объявленных в \\_\\_init\\_\\_()\n",
    "\n",
    "Ниже приведен пример использования keras API для определения двухслойной полносвязной сети. \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определяем на чём обучать модель\n",
    "def set_GPU_param(IS_USED_GPU = True):\n",
    "    return '/device:GPU:0' if IS_USED_GPU else '/cpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__() #Вызываем метод __init__ у базового класса tf.keras.Model    \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = set_GPU_param()\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте трехслойную CNN для вашей задачи классификации. \n",
    "\n",
    "Архитектура сети:\n",
    "    \n",
    "1. Сверточный слой (5 x 5 kernels, zero-padding = 'same')\n",
    "2. Функция активации ReLU \n",
    "3. Сверточный слой (3 x 3 kernels, zero-padding = 'same')\n",
    "4. Функция активации ReLU \n",
    "5. Полносвязный слой \n",
    "6. Функция активации Softmax \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
    "        # should instantiate layer objects to be used in the forward pass.     #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, \n",
    "                                            (5, 5), \n",
    "                                            padding='same', \n",
    "                                            activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, \n",
    "                                            (3, 3), \n",
    "                                            padding='same', \n",
    "                                            activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, \n",
    "                                        activation='softmax')\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
    "        # should use the layer objects defined in the __init__ method.         #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        scores = self.conv1(x)\n",
    "        scores = self.conv2(scores)\n",
    "        scores = self.flatten(scores)\n",
    "        scores = self.fc(scores)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                           END OF YOUR CODE                           #\n",
    "        ########################################################################        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "print(device)\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример реализации процесса обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the MNIST training set and periodically checks\n",
    "    accuracy on the MNIST validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during trainingn\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        i = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_state()\n",
    "            train_accuracy.reset_state()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if i % print_every == 0:\n",
    "                        val_loss.reset_state()\n",
    "                        val_accuracy.reset_state()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(i, epoch + 1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result() * 100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result() * 100))\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.932678699493408, Accuracy: 18.75, Val Loss: 2.363511800765991, Val Accuracy: 24.69999885559082\n",
      "Iteration 100, Epoch 1, Loss: 0.6379953026771545, Accuracy: 80.61571502685547, Val Loss: 0.5589851140975952, Val Accuracy: 81.5999984741211\n",
      "Iteration 200, Epoch 1, Loss: 0.5103380084037781, Accuracy: 84.86473846435547, Val Loss: 0.47028595209121704, Val Accuracy: 85.29999542236328\n",
      "Iteration 300, Epoch 1, Loss: 0.45379912853240967, Accuracy: 86.5604248046875, Val Loss: 0.4456632435321808, Val Accuracy: 86.4000015258789\n",
      "Iteration 400, Epoch 1, Loss: 0.411295622587204, Accuracy: 87.82341003417969, Val Loss: 0.38464611768722534, Val Accuracy: 88.80000305175781\n",
      "Iteration 500, Epoch 1, Loss: 0.38880079984664917, Accuracy: 88.5104751586914, Val Loss: 0.3675382435321808, Val Accuracy: 89.30000305175781\n",
      "Iteration 600, Epoch 1, Loss: 0.3655530512332916, Accuracy: 89.16648864746094, Val Loss: 0.3497719466686249, Val Accuracy: 89.9000015258789\n",
      "Iteration 700, Epoch 1, Loss: 0.34834301471710205, Accuracy: 89.68437957763672, Val Loss: 0.32106640934944153, Val Accuracy: 90.69999694824219\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите трехслойную CNN. В tf.keras.optimizers.SGD укажите Nesterov momentum = 0.9 . \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
    "\n",
    "Значение accuracy на валидационной выборке после 1 эпохи обучения должно быть > 50% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.303586006164551, Accuracy: 9.375, Val Loss: 2.288219690322876, Val Accuracy: 14.30000114440918\n",
      "Iteration 100, Epoch 1, Loss: 0.6931470632553101, Accuracy: 80.12067413330078, Val Loss: 0.5041230916976929, Val Accuracy: 82.80000305175781\n",
      "Iteration 200, Epoch 1, Loss: 0.5150760412216187, Accuracy: 85.20677947998047, Val Loss: 0.36565157771110535, Val Accuracy: 88.80000305175781\n",
      "Iteration 300, Epoch 1, Loss: 0.4171001613140106, Accuracy: 88.06063079833984, Val Loss: 0.2254384607076645, Val Accuracy: 92.79999542236328\n",
      "Iteration 400, Epoch 1, Loss: 0.35133764147758484, Accuracy: 89.89635467529297, Val Loss: 0.2020379900932312, Val Accuracy: 94.0999984741211\n",
      "Iteration 500, Epoch 1, Loss: 0.31177279353141785, Accuracy: 90.98365783691406, Val Loss: 0.1862219274044037, Val Accuracy: 94.4000015258789\n",
      "Iteration 600, Epoch 1, Loss: 0.2806331217288971, Accuracy: 91.8625259399414, Val Loss: 0.20509473979473114, Val Accuracy: 93.5\n",
      "Iteration 700, Epoch 1, Loss: 0.25689271092414856, Accuracy: 92.51738739013672, Val Loss: 0.1686519831418991, Val Accuracy: 93.9000015258789\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = ThreeLayerConvNet(channel_1, \n",
    "                              channel_2, \n",
    "                              num_classes)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, \n",
    "                                        momentum=0.9, \n",
    "                                        nesterov=True)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Использование Keras Sequential API для реализации последовательных моделей.\n",
    "\n",
    "Пример для полносвязной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.77708101272583, Accuracy: 14.0625, Val Loss: 2.403806209564209, Val Accuracy: 20.700000762939453\n",
      "Iteration 100, Epoch 1, Loss: 0.6345235109329224, Accuracy: 80.86324310302734, Val Loss: 0.5187417268753052, Val Accuracy: 83.60000610351562\n",
      "Iteration 200, Epoch 1, Loss: 0.503331184387207, Accuracy: 84.98912048339844, Val Loss: 0.43620529770851135, Val Accuracy: 87.0\n",
      "Iteration 300, Epoch 1, Loss: 0.4450308084487915, Accuracy: 86.79402160644531, Val Loss: 0.4021882712841034, Val Accuracy: 86.69999694824219\n",
      "Iteration 400, Epoch 1, Loss: 0.4013379216194153, Accuracy: 88.11954498291016, Val Loss: 0.35931792855262756, Val Accuracy: 89.0999984741211\n",
      "Iteration 500, Epoch 1, Loss: 0.37838059663772583, Accuracy: 88.81300354003906, Val Loss: 0.3352443277835846, Val Accuracy: 90.5\n",
      "Iteration 600, Epoch 1, Loss: 0.3558100461959839, Accuracy: 89.4914779663086, Val Loss: 0.3236437141895294, Val Accuracy: 90.30000305175781\n",
      "Iteration 700, Epoch 1, Loss: 0.3388758897781372, Accuracy: 89.98974609375, Val Loss: 0.30837008357048035, Val Accuracy: 91.29999542236328\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (28, 28, 1)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, \n",
    "                              activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, \n",
    "                              activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативный менее гибкий способ обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - loss: 0.5441 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.2979 - val_sparse_categorical_accuracy: 0.9160\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2431 - sparse_categorical_accuracy: 0.9308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20824557542800903, 0.9409999847412109]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=64, \n",
    "          epochs=1, \n",
    "          validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишите реализацию трехслойной CNN с помощью tf.keras.Sequential API . Обучите модель двумя способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 2.3240113258361816, Accuracy: 12.5, Val Loss: 2.2113516330718994, Val Accuracy: 26.19999885559082\n",
      "Iteration 100, Epoch 1, Loss: 0.4356260299682617, Accuracy: 86.97401428222656, Val Loss: 0.28096169233322144, Val Accuracy: 91.19999694824219\n",
      "Iteration 200, Epoch 1, Loss: 0.3026847541332245, Accuracy: 90.85820770263672, Val Loss: 0.2984236478805542, Val Accuracy: 90.20000457763672\n",
      "Iteration 300, Epoch 1, Loss: 0.23671585321426392, Accuracy: 92.81561279296875, Val Loss: 0.11650169640779495, Val Accuracy: 96.0\n",
      "Iteration 400, Epoch 1, Loss: 0.19696937501430511, Accuracy: 94.0032730102539, Val Loss: 0.11136055737733841, Val Accuracy: 96.4000015258789\n",
      "Iteration 500, Epoch 1, Loss: 0.1731095165014267, Accuracy: 94.74800109863281, Val Loss: 0.08947378396987915, Val Accuracy: 97.19999694824219\n",
      "Iteration 600, Epoch 1, Loss: 0.15513679385185242, Accuracy: 95.28390502929688, Val Loss: 0.13157851994037628, Val Accuracy: 96.0\n",
      "Iteration 700, Epoch 1, Loss: 0.1426423192024231, Accuracy: 95.65352630615234, Val Loss: 0.10245255380868912, Val Accuracy: 97.0\n"
     ]
    }
   ],
   "source": [
    "def model_init_fn():\n",
    "    model = None\n",
    "    ############################################################################\n",
    "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Conv2D(filters=32, \n",
    "                               kernel_size=(5, 5), \n",
    "                               padding='same', \n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "                                     strides=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(filters=64, \n",
    "                               kernel_size=(5, 5),\n",
    "                               padding='same', \n",
    "                               activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), \n",
    "                                     strides=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1024, \n",
    "                              activation='relu'),\n",
    "        tf.keras.layers.Dense(units=10, \n",
    "                              activation='softmax')\n",
    "        ])\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                            END OF YOUR CODE                              #\n",
    "    ############################################################################\n",
    "    return model\n",
    "\n",
    "learning_rate = 5e-4\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    ############################################################################\n",
    "    # TODO: Complete the implementation of model_fn.                           #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                           END OF YOUR CODE                               #\n",
    "    ############################################################################\n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - loss: 0.7274 - sparse_categorical_accuracy: 0.7960 - val_loss: 0.1923 - val_sparse_categorical_accuracy: 0.9420\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1440 - sparse_categorical_accuracy: 0.9530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12412513792514801, 0.9610000252723694]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=64, \n",
    "          epochs=1, \n",
    "          validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Использование Keras Functional API\n",
    "\n",
    "Для реализации более сложных архитектур сети с несколькими входами/выходами, повторным использованием слоев, \"остаточными\" связями (residual connections) необходимо явно указать входные и выходные тензоры. \n",
    "\n",
    "Ниже представлен пример для полносвязной сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Instantiate the model given inputs and outputs.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 3.5689733028411865, Accuracy: 14.0625, Val Loss: 2.924168109893799, Val Accuracy: 20.0\n",
      "Iteration 100, Epoch 1, Loss: 0.6908491253852844, Accuracy: 79.31620788574219, Val Loss: 0.5610119700431824, Val Accuracy: 81.5\n",
      "Iteration 200, Epoch 1, Loss: 0.5398033261299133, Accuracy: 84.0096435546875, Val Loss: 0.46026119589805603, Val Accuracy: 85.39999389648438\n",
      "Iteration 300, Epoch 1, Loss: 0.4762960970401764, Accuracy: 85.92711639404297, Val Loss: 0.4238292872905731, Val Accuracy: 86.0999984741211\n",
      "Iteration 400, Epoch 1, Loss: 0.4288247525691986, Accuracy: 87.3636245727539, Val Loss: 0.37715601921081543, Val Accuracy: 88.80000305175781\n",
      "Iteration 500, Epoch 1, Loss: 0.40308693051338196, Accuracy: 88.16429138183594, Val Loss: 0.3535812497138977, Val Accuracy: 89.80000305175781\n",
      "Iteration 600, Epoch 1, Loss: 0.37811338901519775, Accuracy: 88.88831329345703, Val Loss: 0.33893123269081116, Val Accuracy: 90.20000457763672\n",
      "Iteration 700, Epoch 1, Loss: 0.3594030439853668, Accuracy: 89.42581939697266, Val Loss: 0.31155890226364136, Val Accuracy: 91.0999984741211\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с архитектурой сверточной сети. Для вашего набора данных вам необходимо получить как минимум 70% accuracy на валидационной выборке за 10 эпох обучения. Опишите все эксперименты и сделайте выводы (без выполнения данного пункта работы приниматься не будут). \n",
    "\n",
    "Эспериментируйте с архитектурой, гиперпараметрами, функцией потерь, регуляризацией, методом оптимизации.  \n",
    "\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующие модели:\n",
    "### 1. CustomConvNet_B_D.\n",
    "   В этой модели используется сверточный слой, за которым следуют слои BatchNormalization, ReLU и Dropout. BatchNormalization используется для нормализации активаций между слоями, что может способствовать обучению более стабильной и эффективной модели. Dropout применяется для случайного исключения некоторых нейронов в процессе обучения с целью предотвращения переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet_B_D(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet_B_D, self).__init__()\n",
    "        channel_1, channel_2, num_classes = 28, 14, 10\n",
    "        dp_rate = 0.2\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0, seed=42)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=channel_1, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1], \n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.dp1 = tf.keras.layers.Dropout(rate=dp_rate)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=channel_2, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1], \n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.dp2 = tf.keras.layers.Dropout(rate=dp_rate)\n",
    "        self.fl = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(units=num_classes,\n",
    "                                        activation='softmax',\n",
    "                                        kernel_initializer=initializer)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = input_tensor\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dp1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dp2(x)\n",
    "\n",
    "        x = self.fl(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CustomConvNet:\n",
    "В этой модели отсутствуют слои BatchNormalization и Dropout, присутствующие в CustomConvNet_B_D.\n",
    "Она содержит только сверточные слои и слои активации ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        channel_1, channel_2, num_classes = 28, 14, 10\n",
    "        dp_rate = 0.2\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0, seed=42)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=channel_1, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1],\n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=channel_2, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1],\n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.fl = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(units=num_classes,\n",
    "                                        activation='softmax',\n",
    "                                        kernel_initializer=initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on MNIST                      #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = input_tensor\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.fl(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CustomConvNet_Sig:\n",
    "В этой модели используется активация сигмоид для всех сверточных слоев вместо ReLU.\n",
    "Сигмоидные активации могут привести к более ограниченному выходу, что может быть полезным в некоторых случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet_Sig(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet_Sig, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on MNIST                      #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        channel_1, channel_2, num_classes = 28, 14, 10\n",
    "        dp_rate = 0.2\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0, seed=42)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=channel_1, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1],\n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=channel_2, \n",
    "                                            kernel_size=[3,3], \n",
    "                                            strides=[1,1],\n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer)\n",
    "        self.fl = tf.keras.layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(units=num_classes,\n",
    "                                        activation='softmax',\n",
    "                                        kernel_initializer=initializer)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on MNIST                      #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = input_tensor\n",
    "        x = self.conv1(x)\n",
    "        x = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = tf.keras.activations.sigmoid(x)\n",
    "\n",
    "        x = self.fl(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Эмпирическим путём выявлена верхняя граница числа эпох, после которой результаты практически не меняются\n",
    "num_epochs = 5 \n",
    "#Определяем параметры эксперимента\n",
    "models = ['CustomConvNet_B_D', 'CustomConvNet', 'CustomConvNet_Sig']\n",
    "optimizers = ['Adam', 'SGD']\n",
    "#learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "learning_rates = [1e-2] #Для ускорения эксперимента используем одно значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_fn(model_name):\n",
    "    if model_name == 'CustomConvNet_B_D':\n",
    "        return CustomConvNet_B_D()\n",
    "    elif model_name == 'CustomConvNet':\n",
    "        return CustomConvNet()\n",
    "    elif model_name == 'CustomConvNet_Sig':\n",
    "        return CustomConvNet_Sig()\n",
    "\n",
    "def optimizer_init_fn(optimizer_name, learning_rate):\n",
    "    if optimizer_name == 'Adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(model_init_fn, optimizer_init_fn, num_epochs):\n",
    "    model = model_init_fn()\n",
    "    optimizer = optimizer_init_fn()\n",
    "    train_part34(model_init_fn, \n",
    "                           optimizer_init_fn, \n",
    "                           num_epochs=num_epochs, \n",
    "                           is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_experiment():\n",
    "    for model, optimizer, lr in itertools.product(models, optimizers, learning_rates):\n",
    "        print(f'\\nModel: {model}, oprimizer: {optimizer}, lr: {lr}')\n",
    "        train_model(lambda: model_init_fn(model), \n",
    "                    lambda: optimizer_init_fn(optimizer, lr), \n",
    "                    num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: CustomConvNet_B_D, oprimizer: Adam, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 3.023545742034912, Accuracy: 4.6875, Val Loss: 24.35333824157715, Val Accuracy: 20.700000762939453\n",
      "Iteration 500, Epoch 1, Loss: 0.5834444165229797, Accuracy: 92.26858520507812, Val Loss: 0.1650119572877884, Val Accuracy: 94.70000457763672\n",
      "Iteration 1000, Epoch 2, Loss: 0.09028811007738113, Accuracy: 97.3271255493164, Val Loss: 0.12753242254257202, Val Accuracy: 96.0\n",
      "Iteration 1500, Epoch 2, Loss: 0.08368509262800217, Accuracy: 97.45536041259766, Val Loss: 0.10774662345647812, Val Accuracy: 96.69999694824219\n",
      "Iteration 2000, Epoch 3, Loss: 0.06987664103507996, Accuracy: 97.84114837646484, Val Loss: 0.13869178295135498, Val Accuracy: 95.70000457763672\n",
      "Iteration 2500, Epoch 4, Loss: 0.06125343590974808, Accuracy: 97.96797943115234, Val Loss: 0.11766186356544495, Val Accuracy: 96.20000457763672\n",
      "Iteration 3000, Epoch 4, Loss: 0.05974874645471573, Accuracy: 98.08187866210938, Val Loss: 0.11902928352355957, Val Accuracy: 96.80000305175781\n",
      "Iteration 3500, Epoch 5, Loss: 0.05624618008732796, Accuracy: 98.2408447265625, Val Loss: 0.13311967253684998, Val Accuracy: 96.30000305175781\n",
      "\n",
      "Model: CustomConvNet_B_D, oprimizer: SGD, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 2.897630214691162, Accuracy: 10.9375, Val Loss: 2.9926555156707764, Val Accuracy: 20.200000762939453\n",
      "Iteration 500, Epoch 1, Loss: 0.3901428282260895, Accuracy: 88.19236755371094, Val Loss: 0.2525939643383026, Val Accuracy: 92.69999694824219\n",
      "Iteration 1000, Epoch 2, Loss: 0.15010057389736176, Accuracy: 95.55850982666016, Val Loss: 0.1723693162202835, Val Accuracy: 94.70000457763672\n",
      "Iteration 1500, Epoch 2, Loss: 0.13298402726650238, Accuracy: 96.02253723144531, Val Loss: 0.1528167426586151, Val Accuracy: 95.30000305175781\n",
      "Iteration 2000, Epoch 3, Loss: 0.09717727452516556, Accuracy: 97.11820220947266, Val Loss: 0.13159775733947754, Val Accuracy: 95.5\n",
      "Iteration 2500, Epoch 4, Loss: 0.08354423195123672, Accuracy: 97.45227813720703, Val Loss: 0.11807810515165329, Val Accuracy: 95.70000457763672\n",
      "Iteration 3000, Epoch 4, Loss: 0.0824468657374382, Accuracy: 97.53511810302734, Val Loss: 0.1124202236533165, Val Accuracy: 96.20000457763672\n",
      "Iteration 3500, Epoch 5, Loss: 0.06982734799385071, Accuracy: 97.90116882324219, Val Loss: 0.10639187693595886, Val Accuracy: 96.30000305175781\n",
      "\n",
      "Model: CustomConvNet, oprimizer: Adam, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 2.981140375137329, Accuracy: 10.9375, Val Loss: 22.042984008789062, Val Accuracy: 22.799999237060547\n",
      "Iteration 500, Epoch 1, Loss: 0.3637760877609253, Accuracy: 90.42227935791016, Val Loss: 0.2997297942638397, Val Accuracy: 90.5999984741211\n",
      "Iteration 1000, Epoch 2, Loss: 0.155899778008461, Accuracy: 95.15292358398438, Val Loss: 0.23274867236614227, Val Accuracy: 92.5\n",
      "Iteration 1500, Epoch 2, Loss: 0.14058582484722137, Accuracy: 95.66114044189453, Val Loss: 0.20253504812717438, Val Accuracy: 93.9000015258789\n",
      "Iteration 2000, Epoch 3, Loss: 0.11271554976701736, Accuracy: 96.52518463134766, Val Loss: 0.24247321486473083, Val Accuracy: 93.30000305175781\n",
      "Iteration 2500, Epoch 4, Loss: 0.10809380561113358, Accuracy: 96.41317749023438, Val Loss: 0.21919038891792297, Val Accuracy: 94.4000015258789\n",
      "Iteration 3000, Epoch 4, Loss: 0.09779388457536697, Accuracy: 96.88166809082031, Val Loss: 0.24616511166095734, Val Accuracy: 93.9000015258789\n",
      "Iteration 3500, Epoch 5, Loss: 0.0824003592133522, Accuracy: 97.31478881835938, Val Loss: 0.2601127326488495, Val Accuracy: 93.0\n",
      "\n",
      "Model: CustomConvNet, oprimizer: SGD, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 2.981140375137329, Accuracy: 10.9375, Val Loss: 4.312099933624268, Val Accuracy: 27.799999237060547\n",
      "Iteration 500, Epoch 1, Loss: 0.3237183690071106, Accuracy: 90.77470397949219, Val Loss: 0.22625654935836792, Val Accuracy: 93.0999984741211\n",
      "Iteration 1000, Epoch 2, Loss: 0.10786978900432587, Accuracy: 96.79521179199219, Val Loss: 0.14211462438106537, Val Accuracy: 95.20000457763672\n",
      "Iteration 1500, Epoch 2, Loss: 0.09329590946435928, Accuracy: 97.31717681884766, Val Loss: 0.14485734701156616, Val Accuracy: 95.0\n",
      "Iteration 2000, Epoch 3, Loss: 0.0674629807472229, Accuracy: 98.0477066040039, Val Loss: 0.12131619453430176, Val Accuracy: 95.70000457763672\n",
      "Iteration 2500, Epoch 4, Loss: 0.05733097344636917, Accuracy: 98.31434631347656, Val Loss: 0.10722676664590836, Val Accuracy: 96.69999694824219\n",
      "Iteration 3000, Epoch 4, Loss: 0.05299542471766472, Accuracy: 98.47306060791016, Val Loss: 0.10605397075414658, Val Accuracy: 96.69999694824219\n",
      "Iteration 3500, Epoch 5, Loss: 0.04480437934398651, Accuracy: 98.71996307373047, Val Loss: 0.10541164875030518, Val Accuracy: 96.5\n",
      "\n",
      "Model: CustomConvNet_Sig, oprimizer: Adam, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 2.614274501800537, Accuracy: 12.5, Val Loss: 41.07435607910156, Val Accuracy: 10.800000190734863\n",
      "Iteration 500, Epoch 1, Loss: 2.5216760635375977, Accuracy: 11.436501502990723, Val Loss: 2.304147481918335, Val Accuracy: 10.800000190734863\n",
      "Iteration 1000, Epoch 2, Loss: 1.2009363174438477, Accuracy: 57.613033294677734, Val Loss: 0.3070373237133026, Val Accuracy: 90.80000305175781\n",
      "Iteration 1500, Epoch 2, Loss: 0.49263980984687805, Accuracy: 83.20365905761719, Val Loss: 0.15831594169139862, Val Accuracy: 95.20000457763672\n",
      "Iteration 2000, Epoch 3, Loss: 0.10157307982444763, Accuracy: 97.01825714111328, Val Loss: 0.15837770700454712, Val Accuracy: 95.20000457763672\n",
      "Iteration 2500, Epoch 4, Loss: 0.08043169975280762, Accuracy: 97.5831298828125, Val Loss: 0.18805259466171265, Val Accuracy: 94.0999984741211\n",
      "Iteration 3000, Epoch 4, Loss: 0.07560369372367859, Accuracy: 97.7329330444336, Val Loss: 0.153975710272789, Val Accuracy: 95.0999984741211\n",
      "Iteration 3500, Epoch 5, Loss: 0.06192706152796745, Accuracy: 98.14788055419922, Val Loss: 0.17519797384738922, Val Accuracy: 94.0\n",
      "\n",
      "Model: CustomConvNet_Sig, oprimizer: SGD, lr: 0.01\n",
      "Iteration 0, Epoch 1, Loss: 2.614274501800537, Accuracy: 12.5, Val Loss: 3.9646458625793457, Val Accuracy: 9.700000762939453\n",
      "Iteration 500, Epoch 1, Loss: 1.0301886796951294, Accuracy: 72.32721710205078, Val Loss: 0.5383638739585876, Val Accuracy: 83.0999984741211\n",
      "Iteration 1000, Epoch 2, Loss: 0.3764404058456421, Accuracy: 89.19548034667969, Val Loss: 0.43875205516815186, Val Accuracy: 88.0\n",
      "Iteration 1500, Epoch 2, Loss: 0.35887470841407776, Accuracy: 89.5450668334961, Val Loss: 0.42067259550094604, Val Accuracy: 87.5999984741211\n",
      "Iteration 2000, Epoch 3, Loss: 0.3249971568584442, Accuracy: 90.56169891357422, Val Loss: 0.4146650731563568, Val Accuracy: 87.5999984741211\n",
      "Iteration 2500, Epoch 4, Loss: 0.30248191952705383, Accuracy: 91.28694152832031, Val Loss: 0.3981545567512512, Val Accuracy: 88.0\n",
      "Iteration 3000, Epoch 4, Loss: 0.30463147163391113, Accuracy: 91.16732025146484, Val Loss: 0.37818294763565063, Val Accuracy: 89.5\n",
      "Iteration 3500, Epoch 5, Loss: 0.2901844382286072, Accuracy: 91.66905212402344, Val Loss: 0.3729762136936188, Val Accuracy: 89.4000015258789\n"
     ]
    }
   ],
   "source": [
    "print_every = 500\n",
    "start_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишите все эксперименты, результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Эксперимент проводился только для $5$ эпох в силу того, что после проведения $3$ эпох обучения заметно, что изменения результатов становятся незначительными. \n",
    "\n",
    "2. Целесообразно использовать оптимизатор Adam, так как в большинстве случаев он превосходит SGD в эффективности.\n",
    "\n",
    "3. Уменьшение скорости обучения привело к медленной, но ожидаемой сходимости модели к тем же фиксированным результатам. Поэтому важно подбирать параметры для обучения оптимальным образом.\n",
    "\n",
    "4. Использование методов регуляризации, таких как Dropout и BatchNormalization, незначительно, но положительно повлияло на качество модели на валидационной выборке.\n",
    "\n",
    "5. Функция активации ReLU показала результаты немного лучше, чем sigmoid.\n",
    "\n",
    "6. Подобные исследования требуют значительного времени, так как происходит перебор большого количества возможных комбинаций параметров, моделей и оптимизаторов.\n",
    "\n",
    "7. Для некоторых наборов параметров был достигнут вполне приемлемый результат: accuracy $\\thickapprox$ $96\\%$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
